# -*- coding: utf-8 -*-
"""understanding-resnet-architecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-KcKVQQlD1ocYsRpp6MEXq54Ig-LnLBz
"""

import torchvision
import torch.nn as nn
import torch
import torch.nn.functional as F
from torchvision import transforms,models,datasets
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
from torch import optim
device = 'cuda' if torch.cuda.is_available() else 'cpu'
import cv2, glob, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from glob import glob
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset

!pip install -q kaggle
from google.colab import files
files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d tongpython/cat-and-dog
!unzip cat-and-dog.zip

train_data_dir = 'training_set/training_set'
test_data_dir = 'test_set/test_set'

class CatsDogs(Dataset):
  def __init__(self, folder):
    # glob: Returns a list with file paths that matches 
    # the path specified (* == all)
    cats = glob(folder + '/cats/*.jpg')
    dogs = glob(folder + '/dogs/*.jpg')
    self.fpaths = cats[:500] + dogs[:500]  # adding two lists
    # Normalizing according to pre-trained model
    self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                         std=[0.229, 0.224, 0.225])
    from random import shuffle, seed
    seed(10);
    # randomize file paths
    shuffle(self.fpaths)
    # list comprehension to generate ground truth
    self.targets = [fpath.split('/')[-1].startswith('dog') for fpath in self.fpaths] # dog=1 & cat=0
  def __len__(self): 
    return len(self.fpaths)
  def __getitem__(self, ix):
    f = self.fpaths[ix] # return the file path
    target = self.targets[ix] # return ground truth
    im = (cv2.imread(f)[:,:,::-1])  # invert channel order to RGB
    im = cv2.resize(im, (224,224))  # resize images
    im = torch.tensor(im/255) # Scale the images (/255) 
    im = im.permute(2,0,1) # permute from (H, W, C) to (C,H,W)
    im = self.normalize(im) # normalize according to pre-trained model 
    # return image and label
    return im.float().to(device), torch.tensor([target]).float().to(device)

data = CatsDogs(train_data_dir)

im, label = data[20]
plt.imshow(im.permute(1,2,0).cpu())
print(label)

def get_model():
  model = models.resnet18(pretrained=True)
  for param in model.parameters():
      param.requires_grad = False
  model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))
  model.fc = nn.Sequential(nn.Flatten(),
  nn.Linear(512, 128),
  nn.ReLU(),
  nn.Dropout(0.2),
  nn.Linear(128, 1),
  nn.Sigmoid())
  loss_fn = nn.BCELoss()
  optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)
  return model.to(device), loss_fn, optimizer

!pip install torch_summary
from torchsummary import summary
model, criterion, optimizer = get_model()
summary(model, torch.zeros(1,3,224,224))

def train_batch(x, y, model, opt, loss_fn):
  model.train()
  prediction = model(x)
  batch_loss = loss_fn(prediction, y)
  batch_loss.backward()
  optimizer.step()
  optimizer.zero_grad()
  return batch_loss.item()

@torch.no_grad()
def accuracy(x, y, model):
  model.eval()
  prediction = model(x)
  is_correct = (prediction > 0.5) == y
  return is_correct.cpu().numpy().tolist()

def get_data():
  train = CatsDogs(train_data_dir)
  trn_dl = DataLoader(train, batch_size=32, shuffle=True, drop_last = True)
  val = CatsDogs(test_data_dir)
  val_dl = DataLoader(val, batch_size=32, shuffle=True, drop_last = True)
  return trn_dl, val_dl

trn_dl, val_dl = get_data()
model, loss_fn, optimizer = get_model()

train_losses, train_accuracies = [], []
val_accuracies = []
for epoch in range(10):
  print(f" epoch {epoch + 1}/10")
  train_epoch_losses, train_epoch_accuracies = [], []
  val_epoch_accuracies = []

  for ix, batch in enumerate(iter(trn_dl)):
      x, y = batch
      batch_loss = train_batch(x, y, model, optimizer, loss_fn)
      train_epoch_losses.append(batch_loss) 
  train_epoch_loss = np.array(train_epoch_losses).mean()

  for ix, batch in enumerate(iter(trn_dl)):
      x, y = batch
      is_correct = accuracy(x, y, model)
      train_epoch_accuracies.extend(is_correct)
  train_epoch_accuracy = np.mean(train_epoch_accuracies)

  for ix, batch in enumerate(iter(val_dl)):
      x, y = batch
      val_is_correct = accuracy(x, y, model)
      val_epoch_accuracies.extend(val_is_correct)
  val_epoch_accuracy = np.mean(val_epoch_accuracies)

  train_losses.append(train_epoch_loss)
  train_accuracies.append(train_epoch_accuracy)
  val_accuracies.append(val_epoch_accuracy)

# Commented out IPython magic to ensure Python compatibility.
epochs = np.arange(10)+1
import matplotlib.ticker as mtick
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
# %matplotlib inline
plt.plot(epochs, train_accuracies, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')
plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))
plt.title('Training and validation accuracy with ResNet18 \nand 1K training data points')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.ylim(0.95,1)
plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) 
plt.legend()
plt.grid('off')
plt.show()