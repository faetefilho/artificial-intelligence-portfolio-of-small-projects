{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/datasets/pre-trained-roberta/kant.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download file from url\n",
    "wget.download('https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter04/kant.txt', 'D:/datasets/pre-trained-roberta/kant.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/datasets/pre-trained-roberta/kant.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Path.glob at 0x000001AB05703990>\n"
     ]
    }
   ],
   "source": [
    "print(str(Path(\".\").glob(\"D:/datasets/pre-trained-roberta/*.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=path, vocab_size=52_000, min_frequency=2, special_tokens=[ \"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\datasets\\\\pre-trained-roberta\\\\KantaiBERT\\\\vocab.json',\n",
       " 'D:\\\\datasets\\\\pre-trained-roberta\\\\KantaiBERT\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"D:\\datasets\\pre-trained-roberta\\KantaiBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer('D:/datasets/pre-trained-roberta/KantaiBERT/vocab.json', 'D:/datasets/pre-trained-roberta/KantaiBERT/merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'i',\n",
       " ',',\n",
       " 'Ä everyone',\n",
       " '!',\n",
       " 'Ä How',\n",
       " 'Ä you',\n",
       " 'Ä are',\n",
       " 'Ä enjoying',\n",
       " 'Ä this',\n",
       " 'Ä presentation',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test the tokenizer\n",
    "tokenizer.encode(\"Hi, everyone! How you are enjoying this presentation!\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see number of tokens\n",
    "tokenizer.encode(\"Hello, everyone! How you are enjoying this presentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end tokens\n",
    "tokenizer._tokenizer.post_processor = BertProcessing((\"</s>\", tokenizer.token_to_id(\"</s>\")), (\"<s>\", tokenizer.token_to_id(\"<s>\")),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets encode the sentence\n",
    "tokenizer.encode(\"Hello, everyone! How you are enjoying this presentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'H',\n",
       " 'ell',\n",
       " 'o',\n",
       " ',',\n",
       " 'Ä everyone',\n",
       " '!',\n",
       " 'Ä How',\n",
       " 'Ä you',\n",
       " 'Ä are',\n",
       " 'Ä enjoying',\n",
       " 'Ä this',\n",
       " 'Ä presentation',\n",
       " '!',\n",
       " '</s>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, everyone! How you are enjoying this presentation!\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# check specs of available GPU\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Unified Device Architecture (CUDA) was developed by NVIDIA to use the parallel computing power of GPUs for general purpose computing.\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('D:/datasets/pre-trained-roberta/KantaiBERT', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filhof17\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "from transformers import LineByLineTextDataset\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='D:/datasets/pre-trained-roberta/kant.txt',\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLATOR\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='D:/datasets/pre-trained-roberta/KantaiBERT',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filhof17\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2f6e335ba34568af24c5d1db3a53f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\venv-gpu\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\ProgramData\\Anaconda3\\envs\\venv-gpu\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5853, 'learning_rate': 4.906437125748503e-05, 'epoch': 0.19}\n",
      "{'loss': 5.6499, 'learning_rate': 4.812874251497006e-05, 'epoch': 0.37}\n",
      "{'loss': 5.0541, 'learning_rate': 4.7193113772455094e-05, 'epoch': 0.56}\n",
      "{'loss': 4.7111, 'learning_rate': 4.625748502994012e-05, 'epoch': 0.75}\n",
      "{'loss': 4.4615, 'learning_rate': 4.532185628742515e-05, 'epoch': 0.94}\n",
      "{'loss': 4.2873, 'learning_rate': 4.438622754491018e-05, 'epoch': 1.12}\n",
      "{'loss': 4.1694, 'learning_rate': 4.345059880239521e-05, 'epoch': 1.31}\n",
      "{'loss': 4.0066, 'learning_rate': 4.251497005988024e-05, 'epoch': 1.5}\n",
      "{'loss': 3.9312, 'learning_rate': 4.157934131736527e-05, 'epoch': 1.68}\n",
      "{'loss': 3.8512, 'learning_rate': 4.06437125748503e-05, 'epoch': 1.87}\n",
      "{'loss': 3.7586, 'learning_rate': 3.970808383233533e-05, 'epoch': 2.06}\n",
      "{'loss': 3.6657, 'learning_rate': 3.877245508982036e-05, 'epoch': 2.25}\n",
      "{'loss': 3.6165, 'learning_rate': 3.783682634730539e-05, 'epoch': 2.43}\n",
      "{'loss': 3.5435, 'learning_rate': 3.6901197604790425e-05, 'epoch': 2.62}\n",
      "{'loss': 3.4791, 'learning_rate': 3.596556886227545e-05, 'epoch': 2.81}\n",
      "{'loss': 3.46, 'learning_rate': 3.502994011976048e-05, 'epoch': 2.99}\n",
      "{'loss': 3.3739, 'learning_rate': 3.409431137724551e-05, 'epoch': 3.18}\n",
      "{'loss': 3.3267, 'learning_rate': 3.3158682634730545e-05, 'epoch': 3.37}\n",
      "{'loss': 3.2985, 'learning_rate': 3.222305389221557e-05, 'epoch': 3.56}\n",
      "{'loss': 3.2564, 'learning_rate': 3.12874251497006e-05, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\venv-gpu\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\ProgramData\\Anaconda3\\envs\\venv-gpu\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2041, 'learning_rate': 3.035179640718563e-05, 'epoch': 3.93}\n",
      "{'loss': 3.1774, 'learning_rate': 2.9416167664670658e-05, 'epoch': 4.12}\n",
      "{'loss': 3.1393, 'learning_rate': 2.8480538922155693e-05, 'epoch': 4.3}\n",
      "{'loss': 3.0893, 'learning_rate': 2.754491017964072e-05, 'epoch': 4.49}\n",
      "{'loss': 3.0979, 'learning_rate': 2.660928143712575e-05, 'epoch': 4.68}\n",
      "{'loss': 3.0665, 'learning_rate': 2.5673652694610778e-05, 'epoch': 4.87}\n",
      "{'loss': 3.0426, 'learning_rate': 2.473802395209581e-05, 'epoch': 5.05}\n",
      "{'loss': 2.9744, 'learning_rate': 2.3802395209580838e-05, 'epoch': 5.24}\n",
      "{'loss': 2.9572, 'learning_rate': 2.286676646706587e-05, 'epoch': 5.43}\n",
      "{'loss': 2.9397, 'learning_rate': 2.1931137724550898e-05, 'epoch': 5.61}\n",
      "{'loss': 2.9151, 'learning_rate': 2.099550898203593e-05, 'epoch': 5.8}\n",
      "{'loss': 2.906, 'learning_rate': 2.0059880239520957e-05, 'epoch': 5.99}\n",
      "{'loss': 2.8419, 'learning_rate': 1.912425149700599e-05, 'epoch': 6.18}\n",
      "{'loss': 2.8293, 'learning_rate': 1.818862275449102e-05, 'epoch': 6.36}\n",
      "{'loss': 2.8049, 'learning_rate': 1.725299401197605e-05, 'epoch': 6.55}\n",
      "{'loss': 2.7926, 'learning_rate': 1.631736526946108e-05, 'epoch': 6.74}\n",
      "{'loss': 2.8058, 'learning_rate': 1.538173652694611e-05, 'epoch': 6.92}\n",
      "{'loss': 2.7739, 'learning_rate': 1.4446107784431137e-05, 'epoch': 7.11}\n",
      "{'loss': 2.7314, 'learning_rate': 1.3510479041916169e-05, 'epoch': 7.3}\n",
      "{'loss': 2.7398, 'learning_rate': 1.2574850299401197e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\venv-gpu\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\ProgramData\\Anaconda3\\envs\\venv-gpu\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7237, 'learning_rate': 1.1639221556886227e-05, 'epoch': 7.67}\n",
      "{'loss': 2.6915, 'learning_rate': 1.0703592814371257e-05, 'epoch': 7.86}\n",
      "{'loss': 2.676, 'learning_rate': 9.767964071856289e-06, 'epoch': 8.05}\n",
      "{'loss': 2.6774, 'learning_rate': 8.832335329341319e-06, 'epoch': 8.23}\n",
      "{'loss': 2.6576, 'learning_rate': 7.896706586826349e-06, 'epoch': 8.42}\n",
      "{'loss': 2.6603, 'learning_rate': 6.961077844311377e-06, 'epoch': 8.61}\n",
      "{'loss': 2.6357, 'learning_rate': 6.0254491017964076e-06, 'epoch': 8.79}\n",
      "{'loss': 2.6322, 'learning_rate': 5.0898203592814375e-06, 'epoch': 8.98}\n",
      "{'loss': 2.6148, 'learning_rate': 4.1541916167664675e-06, 'epoch': 9.17}\n",
      "{'loss': 2.6155, 'learning_rate': 3.218562874251497e-06, 'epoch': 9.36}\n",
      "{'loss': 2.5936, 'learning_rate': 2.282934131736527e-06, 'epoch': 9.54}\n",
      "{'loss': 2.6005, 'learning_rate': 1.3473053892215569e-06, 'epoch': 9.73}\n",
      "{'loss': 2.5991, 'learning_rate': 4.116766467065869e-07, 'epoch': 9.92}\n",
      "{'train_runtime': 5402.8054, 'train_samples_per_second': 316.436, 'train_steps_per_second': 4.946, 'train_loss': 3.2903394824730423, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26720, training_loss=3.2903394824730423, metrics={'train_runtime': 5402.8054, 'train_samples_per_second': 316.436, 'train_steps_per_second': 4.946, 'train_loss': 3.2903394824730423, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pretraining the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model('D:/datasets/pre-trained-roberta/KantaiBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill mask pipeline\n",
    "from transformers import pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"D:/datasets/pre-trained-roberta/KantaiBERT\", \n",
    "    tokenizer=\"D:/datasets/pre-trained-roberta/KantaiBERT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3568747639656067,\n",
       "  'token': 16,\n",
       "  'token_str': ',',\n",
       "  'sequence': 'Immanuel Kant was a German philosopher who is a central figure in modern philosophy. In his writings, the term \"metaphysics\" refers to:,'},\n",
       " {'score': 0.05340297147631645,\n",
       "  'token': 30,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'Immanuel Kant was a German philosopher who is a central figure in modern philosophy. In his writings, the term \"metaphysics\" refers to::'},\n",
       " {'score': 0.03968842700123787,\n",
       "  'token': 263,\n",
       "  'token_str': ' a',\n",
       "  'sequence': 'Immanuel Kant was a German philosopher who is a central figure in modern philosophy. In his writings, the term \"metaphysics\" refers to: a'},\n",
       " {'score': 0.03458130732178688,\n",
       "  'token': 17,\n",
       "  'token_str': '-',\n",
       "  'sequence': 'Immanuel Kant was a German philosopher who is a central figure in modern philosophy. In his writings, the term \"metaphysics\" refers to:-'},\n",
       " {'score': 0.023171335458755493,\n",
       "  'token': 18,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Immanuel Kant was a German philosopher who is a central figure in modern philosophy. In his writings, the term \"metaphysics\" refers to:.'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask our model to think like immanuel Kant\n",
    "fill_mask(\"Immanuel Kant was a German philosopher who is a central figure in modern philosophy. In his writings, the term \\\"metaphysics\\\" refers to: <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13386596739292145,\n",
       "  'token': 483,\n",
       "  'token_str': ' mere',\n",
       "  'sequence': 'Human thinking is a mere phenomenon.'},\n",
       " {'score': 0.08646190166473389,\n",
       "  'token': 629,\n",
       "  'token_str': ' given',\n",
       "  'sequence': 'Human thinking is a given phenomenon.'},\n",
       " {'score': 0.07142633944749832,\n",
       "  'token': 973,\n",
       "  'token_str': ' real',\n",
       "  'sequence': 'Human thinking is a real phenomenon.'},\n",
       " {'score': 0.05899018421769142,\n",
       "  'token': 1393,\n",
       "  'token_str': ' simple',\n",
       "  'sequence': 'Human thinking is a simple phenomenon.'},\n",
       " {'score': 0.030353426933288574,\n",
       "  'token': 468,\n",
       "  'token_str': ' pure',\n",
       "  'sequence': 'Human thinking is a pure phenomenon.'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Human thinking is a <mask> phenomenon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07098536938428879,\n",
       "  'token': 692,\n",
       "  'token_str': ' necessary',\n",
       "  'sequence': 'The human mind is a necessary phenomenon.'},\n",
       " {'score': 0.05616537481546402,\n",
       "  'token': 1393,\n",
       "  'token_str': ' simple',\n",
       "  'sequence': 'The human mind is a simple phenomenon.'},\n",
       " {'score': 0.0540444515645504,\n",
       "  'token': 483,\n",
       "  'token_str': ' mere',\n",
       "  'sequence': 'The human mind is a mere phenomenon.'},\n",
       " {'score': 0.05046757683157921,\n",
       "  'token': 756,\n",
       "  'token_str': ' certain',\n",
       "  'sequence': 'The human mind is a certain phenomenon.'},\n",
       " {'score': 0.04848719388246536,\n",
       "  'token': 2228,\n",
       "  'token_str': ' single',\n",
       "  'sequence': 'The human mind is a single phenomenon.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"The human mind is a <mask> phenomenon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07824818044900894,\n",
       "  'token': 663,\n",
       "  'token_str': ' same',\n",
       "  'sequence': 'The good or bad consequences arising from the performance of an same action'},\n",
       " {'score': 0.02895955927670002,\n",
       "  'token': 774,\n",
       "  'token_str': ' end',\n",
       "  'sequence': 'The good or bad consequences arising from the performance of an end action'},\n",
       " {'score': 0.022305356338620186,\n",
       "  'token': 394,\n",
       "  'token_str': ' object',\n",
       "  'sequence': 'The good or bad consequences arising from the performance of an object action'},\n",
       " {'score': 0.02161231078207493,\n",
       "  'token': 1465,\n",
       "  'token_str': ' infinite',\n",
       "  'sequence': 'The good or bad consequences arising from the performance of an infinite action'},\n",
       " {'score': 0.02143501304090023,\n",
       "  'token': 2114,\n",
       "  'token_str': ' actual',\n",
       "  'sequence': 'The good or bad consequences arising from the performance of an actual action'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"The good or bad consequences arising from the performance of an <mask> action\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gpu",
   "language": "python",
   "name": "venv-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
